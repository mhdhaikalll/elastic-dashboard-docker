input {
 file {
   #https://www.elastic.co/guide/en/logstash/current/plugins-inputs-file.html
   #default is TAIL which assumes more data will come into the file.
   #change to mode => "read" if the file is a compelte file.  by default, the file will be removed once reading is complete -- backup your files if you need them.
   mode => "tail"
   path => "/usr/share/logstash/ingest_data/*"
  #  start_position => "beginning"
  #  sincedb_path => "NUL"  # This will prevent Logstash from keeping track of the file's read state.
  #  add_field => { "ingest_time" => "%{+YYYY-MM-dd HH:mm:ss}" }  # Add a field with the current time
  #  codec => "json"  # Assuming the input files are in JSON format
 }
}


filter {
  mutate {
    add_field => {
      "[dest][location][lat]" => "%{[Destination Geolocation Latitude]}"
      "[dest][location][lon]" => "%{[Destination Geolocation Longitude]}"
    }
    convert => {
      "[dest][location][lat]" => "float"
      "[dest][location][lon]" => "float"
    }
  }

  mutate {
    add_field => {
      "[source][location][lat]" => "%{[Source Geolocation Latitude]}"
      "[source][location][lon]" => "%{[Source Geolocation Longitude]}"
    }
    convert => {
      "[source][location][lat]" => "float"
      "[source][location][lon]" => "float"
    }
  }
}


output {
 elasticsearch {
   index => "logstash-%{+YYYY.MM.dd}"
   hosts=> "${ELASTIC_HOSTS}"
   user=> "${ELASTIC_USER}"
   password=> "${ELASTIC_PASSWORD}"
   cacert=> "certs/ca/ca.crt"
 }

 stdout {
   codec => rubydebug  # This will print the output to the console in a readable format
 }
}
